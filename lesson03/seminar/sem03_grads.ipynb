{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семинар 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Трехмерная геометрия\n",
    "\n",
    "### Теоретическая часть\n",
    "\n",
    "#### Ответьте на вопросы:\n",
    "* Каким уравнением задается прямая на плоскости? В чем отличие уравнений $y = kx + b$ и $ax + by + c = 0$?\n",
    "* Запишите уравнение плоскости в трехмерном пространстве, гиперплоскости в многомерном пространстве. \n",
    "* В пространстве какой размерности задается гиперплоскость из предыдущего вопроса?\n",
    "* Если какой-то из коэффициентов равен 0, что это геометрически означает?\n",
    "* Что означает, что свободный член равен 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Практическая часть: визуализация функции от двух переменных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (11.5, 8.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3d(fun, a=-1, b=1, c=-1, d=1, trace=None):\n",
    "    \"\"\"\n",
    "    Визуализирует функцию fun на квадрате [a, b] x [c, d]\n",
    "    fun : функция, принимающая два аргумента \n",
    "         (np.array одинакового размера) и возвращающая\n",
    "          np.array того же размера со значениями функции\n",
    "          в соответствующих точках\n",
    "    Дополнительно возможно нарисовать ломаную линию из N точек,\n",
    "    лежащую на получившейся поверхности\n",
    "    trace : np.array размера N x 2 - координаты на плоскости,\n",
    "            обозначающие точки ломаной\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "\n",
    "    # Make grid\n",
    "    x1_ = np.linspace(a, b, 100)\n",
    "    x2_ = np.linspace(c, d, 100)\n",
    "    x1, x2 = np.meshgrid(x1_, x2_)\n",
    "    y = fun(x1, x2)\n",
    "\n",
    "    # Plot the surface\n",
    "    ax = fig.add_subplot(1, 1, 1, projection='3d')\n",
    "    ax.plot_surface(x1, x2, y, alpha=0.6)\n",
    "    ax.contour(x1, x2, y, zdir='z', \n",
    "                  offset=y.min(), cmap=cm.coolwarm)\n",
    "    \n",
    "    # Plot 3d line \n",
    "    if trace is not None:\n",
    "        y_trace = fun(trace[:, 0], trace[:, 1])\n",
    "        ax.plot(trace[:, 0], trace[:, 1], y_trace, \"o-\")\n",
    "        ax.set_xlim(x1.min(), x1.max())\n",
    "        ax.set_ylim(x2.min(), x2.max())\n",
    "        ax.set_zlim(y.min(), y.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вам дана функция, визуализирующая поверхности. Ознакомьтесь с ее интерфейсом.\n",
    "\n",
    "Например, отобразим трехмерную параболу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun = lambda x1, x2: x1**2 + x2**2\n",
    "plot_3d(fun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Круги на плоскости показывают проекции линий уровня поверхности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нарисуйте плоскость $y = x_1 + 2 x_2 + 3$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun = lambda x1, x2: # student's code here\n",
    "plot_3d(fun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нарисуйте плоскость, параллельную любой из горизонтальных осей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun = lambda x1, x2: # student's code here\n",
    "plot_3d(fun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нарисуйте плоскость, проходящую через начало координат:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun = lambda x1, x2: # student's code here\n",
    "plot_3d(fun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Градиентный спуск\n",
    "\n",
    "### Теоретическая часть\n",
    "\n",
    "Градиент функции $f(x) = f(x_1, \\dots, x_d)$ от многих переменных в точке $x_0$ - это вектор ее частных производных, вычисленных в точке $x_0$.\n",
    "$$\\nabla_x f \\bigl | _{x_0} = \\biggl(\\frac{\\partial f}{\\partial x_1}, \\dots, \\frac{\\partial f}{\\partial x_d} \\biggr ) \\biggl | _{x_0}$$\n",
    "\n",
    "Разберем два простых примера вычисления градиента в случае функции от двух переменных.\n",
    "\n",
    "#### Задача 1.\n",
    "\n",
    "Найдите градиент линейной функции $f(x) = f(x_1, x_2) = c_1 x_1 + c_2 x_2$ ($c_1$ и $c_2$ - фиксированные числа). \n",
    "\n",
    "#### Задача 2.\n",
    "Найдите градиент квадратичной функции $f(x) = f(x_1, x_2) = c_1 x_1^2 + c_2 x_2^2$ ($c_1$ и $c_2$ - фиксированные числа). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ясно, что минимум такой квадратичной функции достигается в 0.\n",
    "Наша следующая цель - найти этот минимум с помощью градиентного спуска. \n",
    "\n",
    "__Вспомните из лекции:__\n",
    "* Какую (оптимизационную) задачу решает градиентный спуск?\n",
    "* Как работает алгоритм градиентного спуска?\n",
    "* Как выбирать начальную инициализацию в градиентном спуске?\n",
    "* Когда останавливать градиентный спуск?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Практическая часть\n",
    "\n",
    "Воспользуемся кодом квадратичной функции (выше) и обобщим ее на случай произвольных коэффициентов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(x1, x2, c1=1, c2=1):\n",
    "    return c1*(x1**2) + c2*(x2**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эта функция способна обрабатывать x1 и x2 любой размерности, но градиенты будем считать в предположении, что x1 и x2 - числа (так проще)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь надо реализовать градиент функции fun. Напишите код функции вычисления градиента в одной точке согласно описанию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_fun(x1, x2, c1=1, c2=1):\n",
    "    \"\"\"\n",
    "    Функция берет 2 числа, обозначающую точку вычисления градиента,\n",
    "    и возвращает np.array размера (2,) - градиент квадратичной функции\n",
    "    Опциональные аргументы: c1 и c2 - коэффициенты\n",
    "    \"\"\"\n",
    "    ### student's code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверьте правильность (сверьте с формулой):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_fun(x1=0.5, x2=1.5, c1=2, c2=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь реализуем градиентный спуск. \n",
    "Он работает следующим образом: \n",
    "1. сначала инициализируется начальная точка x (это уже сделано)\n",
    "1. затем повторяются итерации:\n",
    "$$x = x - \\alpha \\nabla_x f$$\n",
    "Здесь $\\alpha$ - длина шага.\n",
    "\n",
    "Допишите функцию согласно описанию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_descent(grad_fun, step_size=0.1, num_steps=50):\n",
    "    \"\"\"\n",
    "    Реализует градиентный спуск\n",
    "    Аргументы:\n",
    "    * grad_fun - функция, вычисляющая градиент\n",
    "    * step_size - длина шага\n",
    "    * num_steps - число итераций\n",
    "    \n",
    "    Возвращает np.array размера (num_steps+1) x 2, \n",
    "    (i+1)-й элемент - точка на (i+1)-й итерации,\n",
    "    нулевой элемент - случайная инициализация\n",
    "    \"\"\"\n",
    "    x = np.random.rand(2) * 4 - 2\n",
    "    trace = ### student's code here\n",
    "    \n",
    "    return trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируем функцию (последний элемент должен быть близок к 0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(13)\n",
    "\n",
    "trace = grad_descent(grad_fun)\n",
    "trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы готовы визуализировать градиентный спуск. Для этого передайте нашу траекторию оптимизации в качестве последнего аргумента функции plot_3d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = grad_descent(grad_fun, 0.1, 30)\n",
    "plot_3d(fun, trace=trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Может потребоваться запустить ячейку пару раз, чтобы траекторию было лучше видно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустите оптимизацию несколько раз, чтобы посмотреть, как ведет себя процесс в зависимости от начального приближения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    trace = grad_descent(grad_fun, 0.1, 30)\n",
    "    plot_3d(fun, trace=trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуйте разную длину шага из множества (0.01, 0.1, 0.5, 1). Рекомендуется перед запуском градиентного спуска написать np.random.seed(<число>), чтобы все запуски были из одного начального приближения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### student's code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При маленькой длине шага процесс идет сишком медленно, при большой - может разойтись."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, рассмотрим другую функцию. Оптимизируйте функцию $f(x) = x_1^2 + 5 x_2^2$, пробуя длину шага (0.01, 0.1, 0.2, 0.5):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### student's code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Вытянутую\" функцию сложнее оптимизировать. Именно поэтому данные рекомендуется нормировать перед обучением модели, чтобы избежать таких колебаний при оптимизации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Визуализация траекторий GD и SGD\n",
    "На простом примере разберём основные тонкости, связанные со стохастической оптимизацией."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сгенерируем матрицу объекты-признаки $X$ и вектор весов $w_{true}$, вектор целевых переменных $y$ вычислим как $Xw_{true}$ и добавим нормальный шум:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(13)\n",
    "\n",
    "n_features = 2\n",
    "n_objects = 300\n",
    "batch_size = 10\n",
    "num_steps = 43\n",
    "\n",
    "w_true = np.random.normal(size=(n_features, ))\n",
    "\n",
    "X = np.random.uniform(-5, 5, (n_objects, n_features))\n",
    "X *= (np.arange(n_features) * 2 + 1)[np.newaxis, :]  # for different scales\n",
    "Y = X.dot(w_true) + np.random.normal(0, 1, (n_objects))\n",
    "w_0 = np.random.uniform(-2, 2, (n_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим на полученных данных линейную регрессию для MSE при помощи полного градиентного спуска — тем самым получим вектор параметров."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напомним, что в градиентном спуске значения параметров на следующем шаге получаются из значений параметров на текущем шаге смещением в сторону антиградиента функционала: \n",
    "\n",
    "$$w^{(t)} = w^{(t - 1)} - \\eta_t \\nabla Q(w^{(t - 1)}),$$\n",
    "где $\\eta_t$ — длина шага градиентного спуска.\n",
    "\n",
    "Также напомним, что функционал MSE для матриц выглядит так:\n",
    "$$\n",
    "Q(w) = (y - Xw)^T(y-Xw)\n",
    "$$\n",
    "А соответствующий градиент:\n",
    "$$\n",
    "\\nabla_w Q(w) = \\nabla_w[y^Ty - y^TXw - w^TX^Ty + w^TX^TXw] = 0 - X^Ty - X^Ty + (X^TX + X^TX)w = 2X^T(Xw-y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = w_0.copy()\n",
    "w_list = [w.copy()]\n",
    "step_size = 1e-2\n",
    "\n",
    "for i in range(num_steps):\n",
    "    w -= 2 * step_size * np.dot(X.T, np.dot(X, w) - Y) / Y.shape[0]\n",
    "    w_list.append(w.copy())\n",
    "w_list = np.array(w_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Покажем последовательность оценок параметров $w^{(t)}$, получаемых в ходе итераций. Красная точка — $w_{true}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute level set\n",
    "A, B = np.meshgrid(np.linspace(-2, 2, 100), np.linspace(-2, 2, 100))\n",
    "\n",
    "levels = np.empty_like(A)\n",
    "for i in range(A.shape[0]):\n",
    "    for j in range(A.shape[1]):\n",
    "        w_tmp = np.array([A[i, j], B[i, j]])\n",
    "        levels[i, j] = np.mean(np.power(np.dot(X, w_tmp) - Y, 2))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.title('GD trajectory')\n",
    "plt.xlabel(r'$w_1$')\n",
    "plt.ylabel(r'$w_2$')\n",
    "plt.xlim((w_list[:, 0].min() - 0.1, w_list[:, 0].max() + 0.1))\n",
    "plt.ylim((w_list[:, 1].min() - 0.1, w_list[:, 1].max() + 0.1))\n",
    "plt.gca().set_aspect('equal')\n",
    "\n",
    "# visualize the level set\n",
    "CS = plt.contour(A, B, levels, levels=np.logspace(0, 1, num=20), cmap=plt.cm.rainbow_r)\n",
    "CB = plt.colorbar(CS, shrink=0.8, extend='both')\n",
    "\n",
    "# visualize trajectory\n",
    "plt.scatter(w_true[0], w_true[1], c='r')\n",
    "plt.scatter(w_list[:, 0], w_list[:, 1])\n",
    "plt.plot(w_list[:, 0], w_list[:, 1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиент перпендикулярен линиям уровня. Это объясняет такие зигзагообразные траектории градиентного спуска. Для большей наглядности в каждой точке пространства посчитаем градиент функционала и покажем его направление."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute level set\n",
    "A, B = np.meshgrid(np.linspace(-3, 3, 100), np.linspace(-3, 3, 100))\n",
    "A_mini, B_mini = np.meshgrid(np.linspace(-3, 3, 20), np.linspace(-2, 2, 27))\n",
    "\n",
    "levels = np.empty_like(A)\n",
    "for i in range(A.shape[0]):\n",
    "    for j in range(A.shape[1]):\n",
    "        w_tmp = np.array([A[i, j], B[i, j]])\n",
    "        levels[i, j] = np.mean(np.power(np.dot(X, w_tmp) - Y, 2))\n",
    "        \n",
    "# visualize the level set\n",
    "plt.figure(figsize=(13, 9))\n",
    "CS = plt.contour(A, B, levels, levels=np.logspace(-1, 1.5, num=30), cmap=plt.cm.rainbow_r)\n",
    "CB = plt.colorbar(CS, shrink=0.8, extend='both')\n",
    "        \n",
    "# visualize the gradients\n",
    "gradients = np.empty_like(A_mini)\n",
    "for i in range(A_mini.shape[0]):\n",
    "    for j in range(A_mini.shape[1]):\n",
    "        w_tmp = np.array([A_mini[i, j], B_mini[i, j]])\n",
    "        antigrad = - 2*1e-3 * np.dot(X.T, np.dot(X, w_tmp) - Y) / Y.shape[0]\n",
    "        plt.arrow(A_mini[i, j], B_mini[i, j], antigrad[0], antigrad[1], head_width=0.02)\n",
    "\n",
    "plt.title('Antigradients demonstration')\n",
    "plt.xlabel(r'$w_1$')\n",
    "plt.ylabel(r'$w_2$')\n",
    "plt.xlim((w_true[0] - 1.5, w_true[0] + 1.5))\n",
    "plt.ylim((w_true[1] - .5, w_true[1] + .7))\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализируем теперь траектории стохастического градиентного спуска, повторив те же самые действия, оценивая при этом градиент по подвыборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = w_0.copy()\n",
    "w_list = [w.copy()]\n",
    "step_size = 1e-2\n",
    "batch_size = 10\n",
    "\n",
    "for i in range(num_steps):\n",
    "    sample = np.random.randint(n_objects, size=batch_size)\n",
    "    w -= 2 * step_size * np.dot(X[sample].T, np.dot(X[sample], w) - Y[sample]) / batch_size\n",
    "    w_list.append(w.copy())\n",
    "w_list = np.array(w_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute level set\n",
    "A, B = np.meshgrid(np.linspace(-2, 2, 100), np.linspace(-2, 2, 100))\n",
    "\n",
    "levels = np.empty_like(A)\n",
    "for i in range(A.shape[0]):\n",
    "    for j in range(A.shape[1]):\n",
    "        w_tmp = np.array([A[i, j], B[i, j]])\n",
    "        levels[i, j] = np.mean(np.power(np.dot(X, w_tmp) - Y, 2))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(13, 12))\n",
    "plt.title('SGD trajectory')\n",
    "plt.xlabel(r'$w_1$')\n",
    "plt.ylabel(r'$w_2$')\n",
    "plt.xlim((w_list[:, 0].min() - 0.1, w_list[:, 0].max() + 0.1))\n",
    "plt.ylim((w_list[:, 1].min() - 0.1, w_list[:, 1].max() + 0.1))\n",
    "plt.gca().set_aspect('equal')\n",
    "\n",
    "# visualize the level set\n",
    "CS = plt.contour(A, B, levels, levels=np.logspace(0, 1, num=20), cmap=plt.cm.rainbow_r)\n",
    "CB = plt.colorbar(CS, shrink=0.8, extend='both')\n",
    "\n",
    "# visualize trajectory\n",
    "plt.scatter(w_true[0], w_true[1], c='r')\n",
    "plt.scatter(w_list[:, 0], w_list[:, 1])\n",
    "plt.plot(w_list[:, 0], w_list[:, 1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, метод стохастического градиента «бродит» вокруг оптимума. Для изменения ситуации можно подобрать длину шага градиентного спуска $\\eta_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = w_0.copy()\n",
    "w_list = [w.copy()]\n",
    "step_size_0 = 8e-3\n",
    "num_steps = 300\n",
    "\n",
    "for i in range(num_steps):\n",
    "    step_size = step_size_0 / ((i+1)**0.51)\n",
    "    sample = np.random.randint(n_objects, size=batch_size)\n",
    "    w -= 2 * step_size * np.dot(X[sample].T, np.dot(X[sample], w) - Y[sample]) / batch_size\n",
    "    w_list.append(w.copy())\n",
    "w_list = np.array(w_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute level set\n",
    "A, B = np.meshgrid(np.linspace(-2, 2, 100), np.linspace(-2, 2, 100))\n",
    "\n",
    "levels = np.empty_like(A)\n",
    "for i in range(A.shape[0]):\n",
    "    for j in range(A.shape[1]):\n",
    "        w_tmp = np.array([A[i, j], B[i, j]])\n",
    "        levels[i, j] = np.mean(np.power(np.dot(X, w_tmp) - Y, 2))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(13, 9))\n",
    "plt.title('SGD trajectory')\n",
    "plt.xlabel(r'$w_1$')\n",
    "plt.ylabel(r'$w_2$')\n",
    "plt.xlim((w_list[:, 0].min() - 0.1, w_list[:, 0].max() + 0.1))\n",
    "plt.ylim((w_list[:, 1].min() - 0.1, w_list[:, 1].max() + 0.1))\n",
    "#plt.gca().set_aspect('equal')\n",
    "\n",
    "# visualize the level set\n",
    "CS = plt.contour(A, B, levels, levels=np.logspace(0, 1, num=20), cmap=plt.cm.rainbow_r)\n",
    "CB = plt.colorbar(CS, shrink=0.8, extend='both')\n",
    "\n",
    "# visualize trajectory\n",
    "plt.scatter(w_true[0], w_true[1], c='r')\n",
    "plt.scatter(w_list[:, 0], w_list[:, 1])\n",
    "plt.plot(w_list[:, 0], w_list[:, 1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравнение скоростей сходимости"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Последнее, что хотелось бы продемонстрировать — сравнение, насколько быстро достигают оптимума метод полного и стохастического градиентного спуска. Сгенерируем выборку и построим график зависимости функционала от итерации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generation\n",
    "n_features = 50\n",
    "n_objects = 1000\n",
    "num_steps = 200\n",
    "batch_size = 10\n",
    "\n",
    "w_true = np.random.uniform(-2, 2, n_features)\n",
    "\n",
    "X = np.random.uniform(-10, 10, (n_objects, n_features))\n",
    "Y = X.dot(w_true) + np.random.normal(0, 5, n_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import norm\n",
    "\n",
    "step_size_sgd = 1e-2\n",
    "step_size_gd = 1e-2\n",
    "w_sgd = np.random.uniform(-4, 4, n_features)\n",
    "w_gd = w_sgd.copy()\n",
    "residuals_sgd = [np.mean(np.power(np.dot(X, w_sgd) - Y, 2))]\n",
    "residuals_gd = [np.mean(np.power(np.dot(X, w_gd) - Y, 2))]\n",
    "\n",
    "norm_sgd = []\n",
    "norm_gd = []\n",
    "\n",
    "\n",
    "for i in range(num_steps):\n",
    "    step_size = step_size_sgd / ((i+1) ** 0.51)\n",
    "    sample = np.random.randint(n_objects, size=batch_size)\n",
    "    \n",
    "    w_sgd -= 2 * step_size * np.dot(X[sample].T, np.dot(X[sample], w_sgd) - Y[sample]) / batch_size\n",
    "    residuals_sgd.append(np.mean(np.power(np.dot(X, w_sgd) - Y, 2)))\n",
    "    norm_sgd.append(norm(np.dot(X[sample].T, np.dot(X[sample], w_sgd) - Y[sample])))\n",
    "    \n",
    "    w_gd -= 2 * step_size_gd * np.dot(X.T, np.dot(X, w_gd) - Y) / Y.shape[0]\n",
    "    residuals_gd.append(np.mean(np.power(np.dot(X, w_gd) - Y, 2)))\n",
    "    norm_gd.append(norm(np.dot(X.T, np.dot(X, w_gd) - Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13, 6))\n",
    "plt.plot(range(num_steps+1), residuals_gd, label='Full GD')\n",
    "plt.plot(range(num_steps+1), residuals_sgd, label='SGD')\n",
    "\n",
    "plt.title('Empirial risk over iterations')\n",
    "plt.xlim((-1, num_steps+1))\n",
    "plt.legend()\n",
    "plt.xlabel('Iter num')\n",
    "plt.ylabel(r'Q($w$)')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13, 6))\n",
    "plt.plot(range(num_steps), norm_gd, label='Full GD')\n",
    "plt.plot(range(num_steps), norm_sgd, label='SGD')\n",
    "\n",
    "plt.title('Gradient norm over iterations')\n",
    "plt.xlim((-1, num_steps+1))\n",
    "plt.legend()\n",
    "plt.xlabel('Iter num')\n",
    "plt.ylabel(r'$||\\nabla Q$($w$)||')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, GD буквально за несколько итераций оказывается вблизи оптимума, в то время как поведение SGD может быть весьма нестабильным. Как правило, для более сложных моделей наблюдаются ещё большие флуктуации в зависимости качества функционала от итерации при использовании стохастических градиентных методов. Путём подбора величины шага можно добиться лучшей скорости сходимости, и существуют методы, адаптивно подбирающие величину шага (AdaGrad, Adam, RMSProp)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
